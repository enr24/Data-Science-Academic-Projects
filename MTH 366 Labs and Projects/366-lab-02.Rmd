---
title: "Lab 2"
author: "Emmanuel Rayappa"
date: "Updated `r Sys.Date()`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    theme: cosmo
    code_download: yes
subtitle: 'MTH 366: Machine Learning'
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Answer the questions below (use blockquotes by starting your line with ">" to denote your response). When you're finished:

1.  Change your name in the "author:" space at the top of this document.
2.  Click the "Knit" button at the top to create an HTML/PDF file with your answers.
3.  Review your answers, make any changes as necessary, and re-"Knit".
4.  Save your HTML file and upload to BlueLine.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
```

## Question 1

When the number of input variables $p$ is large, there tends to be a deterioration in the performance of k-nearest neighbors (KNN) and other _local_ approaches that perform prediction using only observations that are _near_ the test observation for which a prediction will be made. This phenomenon is known as the __curse of dimensionality__, and is tied into the fact that non-parametric approaches often perform poorly when $p$ is large.

a) Suppose that we have a set of observations, each with measurements on $p=1$ feature, $X$. Assume that $X$ is _uniformly_, or evenly distributed on $[0, 1]$. We plan to predict a test observation's response using only observations that are within 0.05 units of that test observation. For example, in order to predict the response for a test observation with $X = 0.60$, we'll use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the prediction?
> [(0.05 * 2) / 1] = 0.1(10%)

b) Now suppose we have a set of observations, each with measurements on $p=2$ features, $X_1$ and $X_2$. Assume that both inputs are uniformly distributed on $[0, 1] \times [0, 1]$. Again, we'll predict using only observations that are within 0.05 units of $X_1$ _and_ 0.05 units of $X_2$. On average, what fraction of the available observations will we use to make the prediction?
> [(0.05 * 2) / 1] * [(0.05 * 2) / 1] = 0.01 (1%)


c) What happens if $p=100$? On average, what fraction of the available observations will we use to make the prediction? 
> ([(0.05 * 2) / 1]) ^ 100


## Question 2 (Finish by yourself)

Suppose we take a data set, divide it into a 50-50% training-testing split, and try out two different classification procedures. Logistic regression gets an error rate of 20% on the training data and 30% on the test data. 1-nearest neighbors ($k=1$) gets an average error rate (averaged over both testing and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Explain your reasoning.

> We would want to use the nearest neighbors because that had a lower error rate. 

## Question 3 (Finish part m,n by yourself)

The `BlueJays` data set represents measurements on nine variables for a sample of 123 blue jays captured near Oberlin College. For birds that were captured multiple times, the values were averaged. 

Variable|Description
-----|-------
`BirdID`|	ID tag for bird
`KnownSex`|	Sex coded as F or M
`BillDepth`|	Thickness of the bill measured at the nostril (in mm)
`BillWidth`	|Width of the bill (in mm)
`BillLength`|	Length of the bill (in mm)
`Head`	|Distance from tip of bill to back of head (in mm)
`Mass`	|Body mass (in grams)
`Skull`	|Distance from base of bill to back of skull (in mm)
`Sex`	|Sex coded as 0=female or 1=male

In R, you can access this data set from the `Stat2Data` library. 

```{r}
#install.packages('Stat2Data')
library(Stat2Data)
data(BlueJays)
glimpse(BlueJays)
```

For this data set:

a) Your machine learning models to predict sex based on body measurement should only have _six_ input variables. Explain why.

> We don't need the ID to predict the Sex and also the knowSex column is a duplicated information as our response variable. 

b) Create a testing and training data set, using a 70-30% split. How many observations are in the testing and training data sets?

> There are 87 observations in the Train data and 36 in the Test data

```{r}
set.seed(366)
BlueJays$Sex = as.factor(BlueJays$Sex)
trainID = createDataPartition(BlueJays$Sex, times = 1, p = 0.7, list = FALSE)
Train = BlueJays[trainID,]
Test = BlueJays[-trainID,]
nrow(Train)
nrow(Test)
```


c) Fit a linear discriminant model to predict sex based on body measurements of the blue jays in your training data. Display the final model. Which variables appear to be good "discriminants"?
> The good discrimimants are: BillLength, Head, and Skull.

```{r}
model_lda = train(Sex~BillDepth + BillWidth + BillLength + Head + Mass + Skull,data = Train, method = "lda2" )
model_lda
model_lda$finalModel
```


d)  Build a confusion matrix for the linear discriminant model. Comment on the performance of the model.

> The accuracy of the model in general is about 80%. However, the Specifvicity was much better when compared to the Sensetivity. The model was very good at predicting males correctly. It was not good with predicting females though which means that the model makes more mistakes with females. 

```{r}
confusionMatrix(predict(model_lda, newdata = Test), Test$Sex)
```


e) Plot the decision boundary for your LDA model. Use bill length and body mass as your "axis" variables, and set all other input variables to their mean values. Comment on the plot.
> Generally, this plot has 75% accuray

```{r}
library(klaR)
partimat(Sex~BillLength + Mass, data = BlueJays, method = "lda")

```
```{r}
n_breaks = 100
predA = seq(min(BlueJays$BillLength), max(BlueJays$BillLength), length = n_breaks)
predB = seq(min(BlueJays$Mass), max(BlueJays$Mass), length = n_breaks)
Grid = expand.grid(BillLength = predA, Mass = predB)
head(Grid)
Grid = Grid %>%mutate(BillDepth = mean(BlueJays$BillDepth), 
                      BillWidth = mean(BlueJays$BillWidth),
                      Head = mean(BlueJays$Head),
                      Skull = mean(BlueJays$Skull))

head(Grid)
lda_prediction = predict(model_lda, Grid)
Grid = Grid %>% mutate(pred = lda_prediction)

Grid %>% ggplot(aes(x = Mass, y = BillLength)) + geom_tile(aes(fill = pred), alpha = 0.3) + geom_point(data = BlueJays, aes(x = Mass, y = BillLength, col = Sex))

```


f) Fit a quadratic discriminant model to predict sex based on body measurements of the blue jays in your training data.

```{r}
model_qda = train(Sex ~ BillDepth + BillWidth + BillLength + Head + Mass + Skull, data = Train, method = "qda")
model_qda
```


g) Build a confusion matrix for the quadratic discriminant model. Comment on the performance of the model.
> The model does a pretty good job with correctly predicting males but does a very good job of predicting females correctly.

```{r}
confusionMatrix(predict(model_qda,Test), Test$Sex)

```


h) Plot the decision boundary for your QDA model. Use bill length and body mass as your "axis" variables, and set all other input variables to their mean values. Comment on the plot.
> We have a plot that does not have great apperance and this limits how much information we can take from it. 

```{r}
n_breaks = 100
predA = seq(min(BlueJays$BillLength), max(BlueJays$BillLength), length = n_breaks)
predB = seq(min(BlueJays$Mass), max(BlueJays$Mass), length = n_breaks)
Grid = expand.grid(BillLength = predA, Mass = predB)
head(Grid)
Grid = Grid %>%mutate(BillDepth = mean(BlueJays$BillDepth), 
                      BillWidth = mean(BlueJays$BillWidth),
                      Head = mean(BlueJays$Head),
                      Skull = mean(BlueJays$Skull))

head(Grid)
qda_prediction = predict(model_qda, Grid)
Grid = Grid %>% mutate(pred = qda_prediction)

Grid %>% ggplot(aes(x = Mass, y = BillLength)) + geom_tile(aes(fill = pred), alpha = 0.3) + geom_point(data = BlueJays, aes(x = Mass, y = BillLength, col = Sex))

```


i) Plot the decision boundary for your QDA model. Use bill length and body mass as your "axis" variables, and set all other input variables to their _maximum_ values. Comment on the plot. How has the decision boundary changed?
> The decision boundary has not changed dramatically. 

```{r}
n_breaks = 100
predA = seq(min(BlueJays$BillLength), max(BlueJays$BillLength), length = n_breaks)
predB = seq(min(BlueJays$Mass), max(BlueJays$Mass), length = n_breaks)
Grid = expand.grid(BillLength = predA, Mass = predB)
head(Grid)
Grid = Grid %>%mutate(BillDepth = max(BlueJays$BillDepth), 
                      BillWidth = max(BlueJays$BillWidth),
                      Head = max(BlueJays$Head),
                      Skull = max(BlueJays$Skull))

head(Grid)
qda_prediction = predict(model_qda, Grid)
Grid = Grid %>% mutate(pred = qda_prediction)

Grid %>% ggplot(aes(x = Mass, y = BillLength)) + geom_tile(aes(fill = pred), alpha = 0.3) + geom_point(data = BlueJays, aes(x = Mass, y = BillLength, col = Sex))
```


j) Fit a k-nearest neighbors model to predict sex based on body measurements of the blue jays in your training data. Use at least 10 different values of k in your model building.

```{r}
k = data.frame(k = 5:15)
model_knn = train(Sex ~ BillDepth + BillWidth + BillLength + Head + Mass + Skull, data = Train, method = "knn", tuneGrid = k)
model_knn
```


k) For your model, what is the "optimal" k? Explain how you know, and use a plot to support your answer.

> The optimal k for the data is 15.

```{r}
plot(model_knn)
```

l) Build a confusion matrix for the k-nearest neighbor model (using optimal k). Comment on the performance of the model.
> This model generally has a decent accuray at 75%. This model is just over 50/50 with correctly predicting males while it very accurately predicts femal penguins correctly.

```{r}
confusionMatrix(predict(model_knn, Test), Test$Sex)
```


m) Does it make sense to plot a "decision boundary" for k-nearest neighbors? Explain why or why not. (Hint: Try doing it and see what happens.)

> It does make sense to plot a decision boundary because this can shed light on how accurately predictions occur. 

```{r}
n_breaks = 100
predA = seq(min(BlueJays$BillLength), max(BlueJays$BillLength), length = n_breaks)
predB = seq(min(BlueJays$Mass), max(BlueJays$Mass), length = n_breaks)
Grid = expand.grid(BillLength = predA, Mass = predB)
head(Grid)
Grid = Grid %>%mutate(BillDepth = max(BlueJays$BillDepth), 
                      BillWidth = max(BlueJays$BillWidth),
                      Head = max(BlueJays$Head),
                      Skull = max(BlueJays$Skull))

head(Grid)
knn_prediction = predict(model_knn, Grid)
Grid = Grid %>% mutate(pred = knn_prediction)

Grid %>% ggplot(aes(x = Mass, y = BillLength)) + geom_tile(aes(fill = knn_prediction), alpha = 0.3) + geom_point(data = BlueJays, aes(x = Mass, y = BillLength, col = Sex))
```



n) Which model is "best" for predicting sex based on body measurements? Explain your reasoning. Do you have any hesitations about your model?

> The best model for predicting sex based on body measurement is the QDA Because it does the best job of predicting both Male and Females as accurately as possible. The only hesitancey that I have is the fact that the Kappa and accuracy for that model are not very high.
