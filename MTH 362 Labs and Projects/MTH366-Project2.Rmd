---
title: "Distance between Home and College Campus"
author: "Emmanuel Rayappa"
date: "3/29/2022"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    theme: cosmo
    toc_depth: 2
    toc_float: yes
    code_download: yes
    toc: yes
---

# Introduction to the Data
For this project, I will be focusing on data of over 4,000 college students and how far away they live. This Data set contains 4739 observations and 14 variables and is part of the AER Library in R. Some of the variables that are used in this Data Set are: gender, ethnicity, fcollege (Whether or not the students father went to college), mcollege (Whether or not the students mother went to college), distance (How far away the student's house is from the college), and many more. The reason that this data is of interest to me is because I happen to be someone who lives more than 1000 miles from my college. I was fine with being far away because of the internship opportunities as well as involvement in on-campus activities and clubs. However, not every one has the same reason for being either very close to or far away from their home while at college. I'm hoping that this data set can help explain what can help explain what factors into the college distance for students. 

```{r}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
library(tidyverse)
library(caret)
library(ggplot2)
library(AER)
library(RColorBrewer)
library(dplyr)
```

# Response variable and EDA's
As mentioned in the introduction, the focus of this data set is the distance between the student's college and their home. To get an idea of the data that we are using, we can use EDA's to get an idea about some of the distribution within some of the variables of the data. Some of the variables that we want to look at are the distance variable itself. We can use a box plot with the distance variables and we find that the average distance from people's hometown's to college is approximately 15 miles. We also may want to know what ethnicity are collected in this survey. From running a bar plot, we find that on average, the majority of the data has people of "another ethnicity". We also wan to know whether or not the colleges in the data set are in an urban or non-urban area and upon running another bar plot, we find that most of the colleges in the survey are located in non-urban areas. The skewness in the data is worth noting as it will affect our decision on how much of a split to make with our data. 

```{r, message=FALSE, echo=FALSE}
data("CollegeDistance")
CollegeDistance <- CollegeDistance %>% mutate(distance = distance * 10)
CollegeDistance %>% ggplot(aes(x = urban)) + geom_bar(aes(fill = urban))+xlab("Urban Area or Non-Urban?") + ylab("Number of Colleges in Urban Area")
CollegeDistance %>% ggplot(aes(x = ethnicity)) + geom_bar(aes(fill = ethnicity))+xlab("Ethnicty of participants") + ylab("Total Number of people")
CollegeDistance %>% ggplot(aes(x = distance)) + geom_boxplot()
```
# Creating a Test and Train
As mentioned before, our response variable is distance, so what we need to do is create a Test and Train Data Set. However, in our exploratory data analysis, we noted that two of the variables have some skewness and there is a chance that our distance variable is skewed. Looking at our Box-Plot, we see that the average is only 50 while there are out-liers as far as 200. It's more difficult to see and justify skewness with a box plot, so we can simply plot the distance using a histogram, and upon running the plot, we see very obvious signs of skewness, which means an uneven split of 70-30 would be better.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
CollegeDistance %>% ggplot(aes(x = distance)) + geom_histogram(fill = "Blue")

set.seed(366)
trainIndex = createDataPartition(CollegeDistance$distance, p = 0.7, list = FALSE, times = 1)
Train = CollegeDistance[trainIndex,]
Test = CollegeDistance[-trainIndex,]

preprocess = preProcess(CollegeDistance, method = c("center", "scale"))
TrainTransformed = predict(preprocess, Train)
TestTransformed = predict(preprocess, Test)
```

# First Test: Logistic Regression Model and Interpretation

One of the very first models that we can run is the Logistic Regression Model. The benefit of this model is that it makes predictions based on the response variable that we have established and receive accuracy reports on them. To do this, we simply call for the model, which in this case is called model_lm to be executed. From this test, we can learn about which variables have an influence on the response variable (distance in this case). Upon running this model, we can see that some of the variables that have an influence on the response variable are: urbanyes (Whether or not the school is located in an urban are),unemp (County unemployment rate), and tuition. 
 
```{r, message=FALSE, echo=FALSE}
model_lr = train(distance~., data = Train, method = "lm")
summary(model_lr)
```

We can call for model to be run and we want to focus on the parameter R-squared, which indicates that we have an accuracy of approximately 19%. This is to be expected because we picked a continuous variables as a response variable, which also means that we cannot make a confusion matrix to try and make predictions of any type, We could try and categorize the distances by close and far, but an attempt at doing this showed a very uneven balance between the 2 categories. 
```{r,message=FALSE,echo=FALSE}
model_lr
```

# Second Test: Featured Engineering?

The benefit of conducting feature engineering is that it can help us to work with large data sets and also help us to clean up data and even eliminate variables that may not be necessary, especially if our data set is larger. Unfortunately, we do not have a categorical response variable to work with, therefore we cannot use this test.


# Third Test: Ridge Model

The benefit of using a ridge model is that we can make predictions with any type of variable, regardless of whether or not it's categorical or continuous. Another benefit of this model is that it can help up run multiple scenarios with making predictions and also eliminates multicolinnearity in the data. Upon running the model and also plotting the observed and predicted values, we see that the observed vs predicted isn't very accurate, thus making this model not too good. 

```{r, message=FALSE, echo=FALSE}
alpha = 0
lambda = seq(from = 0, to = 200, length = 11)
grid = expand.grid(alpha, lambda)
colnames(grid) = c("alpha", "lambda")

model_ridge = train(distance~.,data = Train, method = "glmnet", tuneGrid = grid)
model_ridge

ridge_prediction <-predict(model_ridge, newdata = Train)

prediction <- tibble(ridge_prediction = ridge_prediction, observed = TrainTransformed$distance)


prediction %>% ggplot(aes(x = observed, y = ridge_prediction)) + geom_point() + geom_smooth(method = "lm")
```
# Fourth Test: Lasso

The next we can try and run is Lasso Regression. Lasso Regression has the benefit of shrinking or minimizing variables in the data set to avoid over fitting. Up to this point, we don't really have a way of controlling that. We use the same parameters from the Ridge Model on the Lasso model. We see that the predictions are good, and the fit of the model has improved a bit from the ridge model based on the plot. 

```{r,message=FALSE,echo=FALSE}
alpha = 0
lambda = seq(from = 0, to = 1, length = 11)
grid = expand.grid(alpha, lambda)
colnames(grid) = c("alpha", "lambda")

model_lasso = train(distance~.,data = Train, method = "glmnet", tuneGrid = grid)
model_lasso

lasso_prediction <- predict(model_lasso, newdata = Train)

prediction <- tibble(lasso_prediction = lasso_prediction, observed = TrainTransformed$distance)

prediction %>% ggplot(aes(x = observed, y = lasso_prediction)) + geom_point() + geom_smooth(method = "lm")
```

# MARS Model
A Mars (Multivariate Adaptive Regression Splines) model is a model that can be used to model a non-linear relationship between a response variables and other variables within our data-set. This type of model is ideal for this data set, as we noted that while the model is able to do linear regression, the different variables within that model may not necessarily have a linear relationship with on another. We use similar Syntax to the Ridge and Lasso models. To interpret the model, we need to take a look at the predictions in a plot like we did with the previous model. We find here that the predictions are the best from the other models.

```{r,message=FALSE, echo=FALSE}
nprune = 4:5
degree = 1:2
grid = expand.grid(nprune, degree)
colnames(grid) = c("nprune", "degree")
model_mars = train(distance~.,data = Train, method = "earth",tuneGrid = grid )
summary(model_mars)

mars_prediction <- predict(model_mars, newdata = Train)

prediction <- tibble(mars_prediction = mars_prediction, observed = TrainTransformed$distance)

prediction %>% ggplot(aes(x = observed, y = mars_prediction)) + geom_point() + geom_smooth(method = "lm")
```

# Best model

With this data set, we unfortunately were not able to run as many models as we would've liked due to our response variable not being categorical, but continuous. However, we still ran 3 models and found that in general, they give great results but don't necessarily fit as well, but using the model fit isn't the most important statistic that we need. We need the R Sq, which relates to the accuracy of the model. Upon running this code, we find that the Mars model has the highest accuracy (Between the 3 models) and thus we should use it as our final model in this instance. Generally, we would have higher accuracy results if it weren't for the issue with our response variable. 

```{r, message=FALSE, echo=FALSE}
R2(predict(model_ridge, Train), Train$distance)
R2(predict(model_lasso, Train), Train$distance)
R2(predict(model_mars, Train), Train$distance)

R2(predict(model_ridge, Test), Test$distance)
R2(predict(model_lasso, Test), Test$distance)
R2(predict(model_mars, Test), Test$distance)
```


# Conclusion and improvements
From this data set, we can understand that we are able to use a linear model to see which variables carry some sort of influence over our response variable, With that being said however, our response variable must have a categorical response variable as having a continuous one will limit how many test we are able to run and also how good and usable our results are. To improve this type of data, we must use a data set that has a categorical response variable, which will allow for us to run more tests and get more accurate results. We also might want to use a data set that is more recent (Within the last 5-10 years) as this data set is from the 80's and many of the parameters such as unemployment may not be as high or low as it is in this data set.
