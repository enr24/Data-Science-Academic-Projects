---
title: 'Lab 9: US Statewide Crime'
author: "Emmanuel Rayappa"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document: default
  html_document: default
subtitle: 'MTH 365: Intro to Data Science'
---

The data set `us_statewide_crime.csv` contains the following crime statistics for all US states and the District of Columbia in 2010.

Variable|Description
---|---
`State`|State name
`Violent_Crime`|Violent crime rate (per 100,000 residents)
`Murder_Rate`|Murder rate (per 100,000 residents)
`Poverty`|Percent of residents below the poverty rate
`High_School`|Percent of residents with a high school degree
`College`|Percent of residents with a four-year college degree
`Single_Parent`|Percent of children living in single parent families
`Unemployed`|Unemployment rate
`Metropolitan`|Percent of residents living in a metropolitan area

> 1. Download this data set from BlueLine, and import it into RStudio. Use the `glimpse()` function to make sure your data has imported correctly.

```{r}
library(tidyverse)
library(mosaic)
library(infer)
library(mdsr)
library(dslabs)
library(ape)
crime <- read.csv('us_statewide_crime.csv')
glimpse(crime)

```


We'd like to investigate two research questions:

1. Which states are most similar?
2. Which factors can explain most of the "variability" in the data?

> 2. What technique(s) are appropriate for each research question? Explain your reasoning.

One technique that we can use is constructing a tree because we can see which variables are the most important.

Another technique that we can use is clustering because we can see observations that are similar to each other and try to group them together. 
---

## Which states are most similiar?

We'll use both hierarchical clustering and $k$-means clustering to group states based on similarity.

> 3. Create a new data set that contains _only_ the numerical variables, and assign the states' names as the row names of your new data set.

```{r}
newCrimeData <- crime %>% select(-State)
rownames(newCrimeData) <- crime$State
crime_dist <- dist(newCrimeData)

```


> 4. Use your new data set to calculate the "distance" between each states. Display a few rows and columns of your distance matrix.

```{r}
Crime_dist_matrix <- as.matrix(crime_dist)
Crime_dist_matrix[1:4, 1:4]
```

> 5. Use hierarchical clustering to cluster states. How many clusters are there? Which groups of states are similar? Can you explain why these similarities might exist?

There are approximately 4 clusters. I believe that certain states are clustered together because they have a similar crime rates or have certain variabls that are close in value to each other.

```{r}
clusters <- hclust(crime_dist)
plot(clusters, hang=-1, cex=0.7)
```

> 6. Create a fan diagram of your clustering. Use the number of clusters you selected in Question 5 to change the colors of the state labels.

```{r}
library(RColorBrewer)
cols <- brewer.pal(4, 'Set1')
clus2 = cutree(clusters, 4)
plot(as.phylo(clusters), type = "fan", tip.color = cols[clus2],
     label.offset = 1, cex = 0.7)
```

> 7. Use $k$-means clustering to cluster the data into the number of clusters you selected in Question 5. How do the clusters change?

The Clusters change 
```{r}
library(mclust)
k <- 5
two_vars <- newCrimeData %>% select(Murder_Rate, Poverty)
kmeans_cluster <- kmeans(two_vars, centers = k, nstart = 10)
kmeans_cluster
```

---

## Which factors explain most of the variability in the data?

There are likely some strong correlations present in this data! We can use principal components analysis to "group" some of these variables into "factors" that might more broadly address the variability in the data.

> 8. Use principal components analysis to identify the most important components in this data. How much variability is explained by the first few principal components?

There is a lot of variability by the first few principal components (explains about 84% of the variability in the data).
```{r}
library(factoextra)
newCrimeNA <- na.omit(newCrimeData)
noCrime_pca <- prcomp(newCrimeNA[,1:8], center = TRUE, scale = TRUE)
summary(noCrime_pca)
noCrime_pca$rotation
fviz_eig(noCrime_pca)
```


> 9. Based on a screen plot, how many principal components should be "retained" in this data?

According to the screen plot, 2 principal components should be retained in the data.

> 10. Visualize the first two principal components. Which variable(s) provide significant contributions to these components? 

Variables that provided significant contributions to the PCA were: College, Poverty, Single_Parent, Murder_Rate, and Violent_Crime_Rate.
```{r}
fviz_pca_var(noCrime_pca, col.var = "contrib")
```


> 11. Do the principal components have "interpretability"? That is, what does the first PC represent? What does the second PC represent? If the components aren't interpretable, explain your reasoning.

The Principal Components show that there are certain variables that have a strong correlation to one another, which makes interprability fairly easy. However, with larger data sets, this can prove to me more of a challenge.