---
title: "MTH 366 Project 1"
author: "Emmanuel Rayappa"
date: "2/18/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(dplyr)
library(caret)
shopper_data = read.csv("online_shoppers_intention.csv")
shopper_data$Revenue = as.factor(shopper_data$Revenue)
```

```{r, echo = FALSE, warning=FALSE}
#please hide all your R codes in the report with echo = FALSE, only show the R outputs. 
```

# Introduction to Data and Why this is Interesting/Important
For this project, we will be working with data on online shoppers. This data is known as "online_shoppers_intention". This is a data with 12330 observations with 18 different variables. 
A few variables are Month (Month that the visit occurred), Weekend(A Boolean indicating if the visit was on a weekend), and Revenue(A class label which indicates whether or not the visit was finalized with a purchase). 
With this data, we want to try and predict whether a customer will end up shopping with a specific store or not. This question is or particular interest for two reason. The first being that online shopping has become very popular since the rise of Amazon in the mid 2010's while the second is due to the emergence of the Corona virus, which has caused people to be weary of in-person shopping. Another way that this question can be of interest is for the companies themselves. If we can run an analysis of what factors into people shopping at a particular store, they can see what they can what works well as well as what needs to be improved upon. 

# Model the data using Logistic Regression
In order to conduct a proper analysis of the data and understand the variables, we first need to take our current data set and split it into training and testing data. We will use a 50-50 split and also establish a response variable. In this case, we are interested in whether or not a purchase was made, so the response variable in this case will be revenue. 

We can first look at the distribution of shoppers and then determine whether or not there is a even or uneven split. From this graphic, we can see that there are more non-shoppers than there are shoppers. 

```{r}
shopper_data %>% ggplot(aes(x = Revenue)) + geom_bar(aes(col = Revenue))
```
From this data set, we can see that there are more non-shoppers (FALSE) than there are shoppers (TRUE). So with this data set, we can use an uneven split with the data (70-30%)
```{r, echo = FALSE, warning=FALSE}
set.seed(366)

trainIndex = createDataPartition(shopper_data$Revenue, p = 0.7, list = FALSE, times = 1)

Train = shopper_data[trainIndex,]
Test = shopper_data[-trainIndex,]
```

Now that we have established a response variable, we want to see what variables are significant in relation to Revenue. To do this, we can create a Logistic Regression Model.

```{r, echo = FALSE, warning = FALSE}
model_lr = train(Revenue~., data = Train, method = "glm", family = "binomial")
summary(model_lr)
```
To see how this model work, we can plot revenue with one of the variables that this model highlights as important. Here, we can see what kind of correlation exists between the Revenue and Exit Rate. From the plot, we can see that there is some correlation between Revenue and ExitRate in that if a person doesn't buy anything, the average exit rate is higher than if a person does make a purchase. 

We can repeat a similar process and we find that ultimately, the average of PageValues is higher for people who ultimately shop vs people who do not shop. 

```{r, echo=FALSE}
Train %>% ggplot(aes(x = Revenue, y = ExitRates)) + geom_boxplot(aes(col = Revenue))

Train %>% ggplot(aes(x = Revenue, y = PageValues)) + geom_boxplot(aes(col = Revenue))

```
However, these visualizations do not necessarily help us with Informational Statistics. We only know about averages based on variables, but don't know anything about how shoppers are predicted besides. In order to do this, we simply call for the model (model_lr) to be executed.
```{r,echo=FALSE,warning=FALSE}
model_lr

```
From the logistic regression model, we saw that the variables that have a significant impact on the Exit Rate, PageValue, MonthDec, MonthMar, MonthMay, VisitorTypeReturning_Visitor. Now we want to see how accurate this model is using the Kappa (Difference between the observed accuracy and expected accuracy) and Accuracy. In this case, the accuracy is 88.65% while the Kappa is 46.7%.


With this in mind, we can use these variables and try and make predictions using the information that we have learned from the Logistic Regression Model using a confusion matrix. 

```{r, echo=FALSE, warning=FALSE}
confusionMatrix(data = predict(model_lr, newdata = Train), reference = Train$Revenue)
```
From this model, we tend to do a good job of predicting that people will not shop based on the variables as indicated by the Sensitivity (97.7%). However, this model does not do a good job of correctly predicting that people will shop based on the variables as indicated by the specificity (38.92%). So this model is good for predicting non-shoppers, but not as good with  predicting shoppers. 


# Modeling with Naive Bayes
As we saw with the Logistic Regression Model, we can correctly predict that people will not shop based on a few factors. The results of this model state that we can correctly predict who will shop, but not be able to correctly predict who will shop. So the Logistic Regression Model is not necessarily our best option. If there is another model that could work, it would be the Naive Bayes, which is another type of model that is great for making predictions.

```{r, echo=FALSE, warning=FALSE}
model_nb = train(Revenue~., data = Train, method = "naive_bayes")
summary(model_nb)
```
From the initial model, we learn the portion of False and True (Did the customer end up purchasing something or not)in the data; which in this case is 84.5% being false and 15.5% being true. However, this does not tell us enough about the accuracy of the test. We need the Kappa and Accuracy once again.

```{r, echo=FALSE,warning=FALSE}
set.seed(366)
model_nb

```
The accuracy for this model is at 67.5% while the Kappa is at 26.16%, which means that we aren't predicting people who do shop correctly. To see whether there is an accuracy issue, we can run a confusion Matrix and see what the Sensitivity and specificity are.

```{r,echo=FALSE,warning=FALSE}
confusionMatrix(data = predict(model_nb, data = Train), reference = Train$Revenue)
```
So based off the results of this test, we have a better specificity of 66.02% and a very good sensitivity of 91.32%. So generally ,this model is good for predicting who will not shop correctly, but not as good with predicting who does shop correctly. 

One other model we can attempt to run is a quadratic discriminant model and see what the Accuracy and Kappa are for this model.

Based on this mode, we have an accuracy of 71.8% but a Kappa of just 31.7% which once again hows that we can accurately predict non-shoppers, but stuggle to predict shoppers.

```{r, message=FALSE, warning=FALSE}
model_qda = train(Revenue~., data = Train, method = "qda" )
model_qda
```


# Which model is best/improvents for future project.

Based off the two models that we have run, a Naive Bayes would be the best model to use for predicting who shops vs who doesn't. The Logistic regression Model is not good with predicting who shops correctly while the Naive Bayes is better (but not by much). Generally, we find that we cannot do a good jo

To improve this project, we could maybe use a smaller data set which has less variables as this can make plotting EDA's easier and could also improve the accuracy of correctly predicting who will actually end up shopping online.
