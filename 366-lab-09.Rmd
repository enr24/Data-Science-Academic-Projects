---
title: "Lab 9"
author: "Emmanuel Rayappa"
date: "Updated `r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: cosmo
    code_download: yes
  pdf_document:
    toc: yes
subtitle: 'MTH 366: Machine Learning'
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Answer the questions below (use blockquotes by starting your line with ">" to denote your response). When you're finished:

1.  Change your name in the "author:" space at the top of this document.
2.  Click the "Knit" button at the top to create an PDF/HTML file with your answers.
3.  Review your answers, make any changes as necessary, and re-"Knit".
4.  Save your PDF/HTML file and upload to BlueLine.

## Question 1 (complete by yourself)

In this question, you'll perform $k$-means clustering manually, with $K=2$, on a small example with $n=10$ observations and $p=2$ features. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
x <- c(1, 1, 0, 5, 6, 4, 2, 1, 2, 5)
y <- c(4, 3, 4, 1, 2, 0, 2, 4, 1, 3)
data <- tibble(x, y)
```

a) Plot the observations.
```{r}
data %>% ggplot(aes(x = x, y = y))+ geom_point()
```

b) Randomly assign a cluster label to each observation. You can use the `sample()` command to do this in R, or even flip a coin. Report the cluster labels for each observation.

```{r}
data = data %>% mutate(cluster = sample(1:2, 10, replace = TRUE))
head(data)
```

c) Compute the centroid for each cluster.
```{r}
set.seed(366)
centroid = data %>% group_by(cluster) %>%summarize(centroid.x = mean(x), centroid.y = mean(y))
centroid

```

d) Plot the clusters and add the centroids.
```{r, message=FALSE}
library(factoextra)
fviz_cluster(list(data = data, cluster = data$cluster), stand = FALSE)
```

e) Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.
```{r}
fviz_cluster(list(data = data, cluster = data$cluster), stand = FALSE)
for (i in 1:10){
  dist = sqrt((as.numeric(data[i,1]) - centroid$centroid.x)^2 +
    (as.numeric(data[i,2]) - centroid$centroid.y)^2)
  if (dist[1] < dist[2]){
    data$cluster[i]= 1
  }
  else{
    data$cluster[i] = 2
  }
  print(data$cluster[i])
}
```

f) Repeat steps c)-e) until the answers obtained stop changing.

g) Plot your final clusters.
```{r}
fviz_cluster(list(data = data, cluster = data$cluster), stand = FALSE)
```



## Question 2

The data set `bob_ross` in the `fivethirtyeight` package contains a series of indicator variables for different elements that might be present in a Bob Ross painting.

For more info about this data: https://fivethirtyeight.com/features/a-statistical-analysis-of-the-work-of-bob-ross/

![Some samples of Bob Ross' work.](https://fivethirtyeight.com/wp-content/uploads/2014/04/ross-cabin.jpg?w=1150)

In this exercise, we'll use clustering to group the paintings into similar groupings.

```{r, message=FALSE, warning=FALSE}
library(fivethirtyeight)
data(bob_ross)
head(bob_ross)
bob_ross2 <- bob_ross %>% dplyr::select(-c(episode, season, episode_num, title, lakes))
library(caret)
nearZeroVar(bob_ross2, saveMetrics = TRUE)
```

a) Set the episode as row names.
```{r}
bob_ross2 = as.data.frame(bob_ross2)
rownames(bob_ross2) = bob_ross$episode
```

b) What is the "optimal" number of clusters for this data using hierarchical clustering? (Hint: Set `k.max` to at least 50.)
```{r}
library(factoextra)
fviz_nbclust(bob_ross2, FUN = hcut, method = "wss", k.max = 50)
```

c) Use hierarchical clustering to group the Bob Ross paintings into ten clusters.
```{r}
dist = dist(bob_ross2, method = "euclidean")
h_clusters = hclust(dist)
```

d) Plot the data by clusters. (Hint: Use the code below to help.) What do the clusters have in common?
```{r}
plot(h_clusters, cex = 0.3)
rect.hclust(h_clusters, k = 10)

clus = cutree(h_clusters, k = 10)
fviz_cluster(list(data = bob_ross2, clusters = clus))
bob_ross2[which(clus == 6), ]
```


## Question 3

The World Health Organization released the results of the Global Information System on Alcohol and Health (GISAH) study in 2010. This data recorded average servings of beer, spirits (liquor), wine, and pure alcohol per person for 193 countries. Which countries have similiar drinking habits?

https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/

```{r}
library(fivethirtyeight)
data(drinks)
glimpse(drinks)
```

a) Set the country as the row names for this data.
```{r}
drinks = as.data.frame(drinks)
rownames(drinks) = drinks$country
drinks$country = NULL
head(drinks)
```

b) Fit a k-means clustering model with $k=4$. Visualize the clusters. Are they distinct in terms of their principal components? Which countries are similar, and which are different?

```{r}
cluster4 = kmeans(drinks, center = 4)
fviz_cluster(list(data = drinks, cluster = cluster4$cluster))
```

c) Fit a k-means clustering model with $k=6$. Visualize the clusters. Are they distinct in terms of their principal components? Which countries are similar, and which are different?
```{r}
cluster6 = kmeans(drinks, center = 6)
fviz_cluster(list(data = drinks, cluster = cluster6$cluster))
```

d) Fit a hierarchical clustering model. "Cut" the tree at $k=4$ clusters. Describe the groupings.
```{r}
dist = dist(drinks, method = "euclidean")
hcluster = hclust(dist)

plot(hcluster, cex = 0.3)
rect.hclust(hcluster, k = 4, border = 2:5)
clus = cutree(hcluster, k =4)
drinks[which(clus == 3),]
```

e) "Cut" the hierarchical clustering tree at $k=6$ clusters.Describe the groupings.
```{r}
plot(hcluster, cex = 0.3)
rect.hclust(hcluster, k = 6, border = 2:5)
clus = cutree(hcluster, k = 6)
drinks[which(clus == 1:4),]
```

f) What is the "optimal" number of clusters for this data using hierarchical clustering?
> The optimal number is 4

```{r}
fviz_nbclust(drinks, FUN = hcut, method = "wss", k.max = 10)
```

g) Choose the "optimal" clustering. Plot each variable in the data, separated by cluster. What does this tell you about what the clusters represent?
```{r}
fviz_cluster(list(data = drinks, cluster = cluster4$cluster)) + facet_wrap(~cluster, scales = "free")
```



## Question 4

Last week we used several examples from Tidy Tuesday to explore principal components. We'll use the same data sets to explore clustering techniques.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(factoextra)
```

![](https://github.com/rfordatascience/tidytuesday/raw/master/static/tt_logo.png)



### Instructions

For each data set:

a). Use k-means clustering to cluster your data into two clusters. If possible, what do the observations in each cluster represent?
b). Use k-means clustering to cluster your data into four clusters. If possible, what do the observations in each cluster represent?
c). __Re-cluster__ your data into four clusters, using a different seed number. How stable are the k-means clusters?
d). Use hierarchical clustering to split your data into four clusters. 
e). How similar are the four clusters from k-means clustering to the four clusters from hierarchical clustering?
f). Using "within cluster sums of squares", how many clusters are optimal? Support your reasoning.

### Example 1: Plastic pollution

Read more about this data:

> https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-01-26/readme.md

```{r, warning=FALSE, message=FALSE}
plastics <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-26/plastics.csv')

head(plastics)
```

### Example 2: Makeup shades

Read more about this data:

> https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-03-30/readme.md

```{r, warning=FALSE, message=FALSE}
allShades <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-30/allShades.csv')

head(allShades)
```
```{r}
shades2 = allShades%>% dplyr::select(hue, sat, lightness)
shades2 = shades2[complete.cases(shades2),]
shades2 = shades2[-2791,]
kcluster = kmeans(shades2, centers = 4)
dist = dist(shades2, method = "euclidean")
hcluster = hclust(dist)
plot(hcluster, cex = 0.5)
rect.hclust(hcluster, k = 4, border = 2:5)
fviz_cluster(list(data = shades2, cluster = kcluster$cluster))
#shades2 = shades2 %>% mutate(clusters_kmeans = kcluster$cluster, clusters_hclust = cutree(hcluster, k = 4))
#shades2 %>% ggplot(aes(x = clusters_kmeans)) + geom_bar(aes(fill = as.factor(clusters_hclust)))
shades2[2785:2793,]
```

### Example 3: Deforestation in Brazil

Read more about this data:

> https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-06/readme.md

```{r, warning=FALSE, message=FALSE}
brazil_loss <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-04-06/brazil_loss.csv')

head(brazil_loss)
```

```{r}
brazil2 = brazil_loss [4:14]
brazil2 = brazil2[complete.cases(brazil2), ]

kcluster = kmeans(brazil2, centers = 4)
dist = dist(brazil2, method = "euclidean")
hcluster = hclust(dist)
plot(hcluster, cex = 0.5)
rect.hclust(hcluster, k = 4, border = 2:5)
fviz_cluster(list(data = brazil2, cluster = kcluster$cluster))
brazil2 = brazil2 %>% mutate(clusters_kmeans = kcluster$cluster, clusters_hclust = cutree(hcluster, k = 4))
brazil2 %>% ggplot(aes(x = clusters_kmeans)) + geom_bar(aes(fill = as.factor(clusters_hclust)))
```

### Example 4: Canadian wind turbines

Read more about this data:

> https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-10-27/readme.md

```{r, warning=FALSE, message=FALSE}
wind_turbine <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-27/wind-turbine.csv')

head(wind_turbine)
```