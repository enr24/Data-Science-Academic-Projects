---
title: "Lab 1"
author: "Emmanuel Rayappa"
date: "Updated `r Sys.Date()`"
output:
  pdf_document:
  html_document:
    toc: yes
    toc_float: yes
    theme: cosmo
    code_download: yes
subtitle: 'MTH 366: Machine Learning'
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

Answer the questions below (use blockquotes by starting your line with ">" to denote your response). When you're finished:

1.  Change your name in the "author:" space at the top of this document.
2.  Click the "Knit" button at the top to create an HTML/PDF file with your answers.
3.  Review your answers, make any changes as necessary, and re-"Knit".
4.  Save your file and upload to BlueLine.

```{r}
library(tidyverse)
library(caret)
```

## Question 1

Explain whether each scenario is a _classification_ or _regression_ problem, and indicate whether we are likely interested in inference or prediction. Provide $n$ and $p$.

a) We collect a set of data on the top 500 firms in the US. For each firm, we record profit, number of employees, industry, and the CEO salary. We are interested in understanding which factors affect CEO salary.

> This would be a case of regresion where we are likely interested in inferences. n = 500
p = 4.

b) We are considering launching a new product and want to know whether it will be a success or failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. 

> This would be a case of classification where we are likely interested in prediction. n = 20, p = 14

c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence, we collect weekly data for all of 2019. For each week, we record the % change in USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

> This would be a case of regression where we are likely interested in prediction.
n = 52 , p = 4

## Question 2

One of the classic benchmarking data sets in machine learning is the Pima Indians data set. This data set was originally produced by the National Institute of Diabetes and Digestive and Kidney Diseases. The goal of this data is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the data set. Several constraints were placed on the selection of these instances from a larger data base: particularly, all patients are females at least 21 years old of Pima Indian heritage.

In R, you can access this data set from the `mlbench` library. 

```{r}
#install.packages('mlbench')
library(mlbench)
data(PimaIndiansDiabetes)
head(PimaIndiansDiabetes)
```

Variable|Description
-------|---------
`pregnant`|	Number of times pregnant
`glucose`|	Plasma glucose concentration (glucose tolerance test)
`pressure`|	Diastolic blood pressure (mm Hg)
`triceps`|	Triceps skin fold thickness (mm)
`insulin`|	2-Hour serum insulin (mu U/ml)
`mass`|	Body mass index (weight in kg/(height in m)\^2)
`pedigree`|	Diabetes pedigree function
`age`|	Age (years)
`diabetes`|	Class variable (test for diabetes)

For this assignment, your task is to build two machine learning models to classify patients as positive or negative for diabetes, and compare their performance.

a) First, build a testing and training data set using a 70-30 split. How many observations are in the testing and training data sets? 

```{r}
set.seed(366)

trainIndex = createDataPartition(PimaIndiansDiabetes$diabetes, p = 0.7, list = FALSE, times = 1)
trainIndex2 = sample(1:nrow(PimaIndiansDiabetes), ceiling(nrow(PimaIndiansDiabetes) * 0.7))

Train = PimaIndiansDiabetes[trainIndex,]
Test = PimaIndiansDiabetes[-trainIndex,]
```

> 

b) Is your testing/training split balanced or unbalanced? Give an advantage for each type of split.

> The balance split is good for the case when the group ratios are staedy. However, if the group ratio is to change later, unbalanced is better.

c) Make a scatterplot of glucose and blood pressure using the training data, and use the plotting color to indicate whether or not a person in the training data has diabetes. Based on your plot, does it look like glucose and blood pressure will be useful input variables? Explain why or why not.

> Based on the plot, it seems that the people with diabetes tend to have higher glucose, this glucose should be a useful input variable. Hwoever, there is no big change on blood pressure regarding wheter they have diabetes or not

```{r}
Train %>% ggplot(aes(x = glucose, y = pressure)) + geom_point(aes(col = diabetes))
```


d) Use your training data to build a __logistic regression model__. Which variables are "significant" in the logistic regression model.

> NUmber of times pregnant, plasma glucose

```{r}
model_lr = train(diabetes~., data = Train, method = "glm", family = "binomial")
summary(model_lr)
```


e) For the training data, what is the "accuracy" and "kappa" of the logistic regression model? What do these terms represent?

> Accuray: 0.757, Kappa: 0.449. Accuray is the observed accuracy and kappa is the difference between observed and expected accuracy.

```{r}
model_lr

```


f) Create a confusion matrix for the logistic regression model using your testing data. What does the confusion matrix tell you about the model's performance in terms of the "true negative rate" and "true positive rate"?

> The true positive rate is the sensitivity and the true negative rate is the specivicty. The sensitivity in this case is relatively higher value compared to the specivicity.Since the positive class is negative, which means that we did a good predicition on the people without diabetes, but not so good for the people with diabetes.

```{r}
confusionMatrix(data = predict(model_lr, newdata = Test), reference = Test$diabetes)
```


g) Use your training data to build a __naive Bayes classification model__. What are the "a priori" probabilities in your model?

> The proportion of group size: is 0.6506 for positive and 0.3494 of negative

```{r}
model_nb = train(diabetes~., data = Train, method = "naive_bayes")
summary(model_nb)
```


h) For the training data, what is the "accuracy" and "kappa" of the naive Bayes classification model?

> Accuracy with kernel estimation: 0.7532 and the kappa :0.448. 
FALSE     Kappa: 0.7422570  Accuray: 0.4189507

```{r}
model_nb
```

i) Create a confusion matrix for the naive Bayes classification model using your testing data. What does the confusion matrix tell you about the model's performance in terms of the "true negative rate" and "true positive rate"?

> WE still have a high value on true positive rate while we have a lower true negative rate, meaning that the rate the negative test is not good.

```{r}
confusionMatrix(data = predict(model_nb, newdata = Test), reference = Test$diabetes)
```


j) Of the models you considered, which is "best"? Explain your reasoning.

> We care about sensitivity and specificity equally, logistice regression tends to be better. However, making mistakes with probabilities with diabetes are more serious than for people without, which mean we specivicity is more important, thus Naive Bayes shoudl be prefered.

k) How often do the predictions from the two models agree? How often do they disagree? What can the disagreements tell you?

> 86.09% predicitions are the same. There are 23 observations that are predicted as positive while logistice predicts them as negative. There are 9 observations that are predicted as negative while logistice predicts them as positive.

```{r}
confusionMatrix(data = predict(model_nb, newdata = Test), reference = predict(model_lr, newdata = Test))
```

## Question 3

The COVID-19 case surveillance system database includes individual-level data reported to U.S. states and autonomous reporting entities, including New York City and the District of Columbia (D.C.), as well as U.S. territories and states. On April 5, 2020, COVID-19 was added to the Nationally Notifiable Condition List and classified as “immediately notifiable, urgent (within 24 hours)” by a Council of State and Territorial Epidemiologists (CSTE) Interim Position Statement (Interim-20-ID-01). CSTE updated the position statement on August 5, 2020 to clarify the interpretation of antigen detection tests and serologic test results within the case classification. The statement also recommended that all states and territories enact laws to make COVID-19 reportable in their jurisdiction, and that jurisdictions conducting surveillance should submit case notifications to CDC. COVID-19 case surveillance data are collected by jurisdictions and shared voluntarily with CDC.

For more information:

https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf

The deidentified data in the public use dataset include demographic characteristics, exposure history, disease severity indicators and outcomes, clinical data, laboratory diagnostic test results, and comorbidities. All data elements can be found on the COVID-19 case report form located at www.cdc.gov/coronavirus/2019-ncov/downloads/pui-form.pdf.

You can download the CSV file from BlueLine.

__Note__: This data set is somewhat large (~1 million cases). BlueLine wouldn't let me upload it. So instead, we'll use a data set with only the complete cases.

### Variable descriptions

Name|Description
------|-----
`cdc_report_dt`|Date CDC reported
`pos_spec_dt`|Date of first positive specimen collection
`onset_dt`|Date of symptom onset
`current_status`|Laboratory-confirmed case or probable case
`sex`|Patient gender
`age_group`|Age group (categories)
`race_ethnicity`|Combined race and ethnicity (CDC classifications)
`hosp_yn`|Was the patient hospitalized?
`icu_yn`|Was the patient admitted to an intensive care unit (ICU)?
`death_yn`|Did the patient die as a result of this illness?
`medcond_yn`|Did the patient have any underlying medical conditions and/or risk behaviors?

### Task

Your task is to use the public use COVID-19 database to:

1) Generate a balanced 80-20 training and testing split
2) Fit a particular model to classify case outcomes (see model assignments below)
3) Evaluate your model's performance on the testing data

Group|Output variable|Input variables|Method
----|----|----|-----
1|`icu_yn`|`current_status`, `sex`, `age_group`, `race_ethnicity`, `medcond_yn`|Logistic regression
2|`icu_yn`|`current_status`, `sex`, `age_group`, `race_ethnicity`, `medcond_yn`|Naive Bayes

3|`death_yn`|`current_status`, `sex`, `age_group`, `race_ethnicity`, `medcond_yn`|Logistic regression
4|`death_yn`|`current_status`, `sex`, `age_group`, `race_ethnicity`, `medcond_yn`|Naive Bayes

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(caret)
library(patchwork)

COVID_complete <- read.csv("COVID_complete.csv",stringsAsFactors=TRUE)

```

```{r}
# Your work here
set.seed(366)

trainIndex = createDataPartition(COVID_complete$icu_yn, p = 0.8, list = FALSE, times = 1)

Train_2 = COVID_complete[trainIndex,]
Test_2 = COVID_complete[-trainIndex,]


model_nb2 = train(icu_yn ~ current_status + sex + age_group + race_ethnicity + medcond_yn, data = Train_2,method = "naive_bayes")

model_nb2

confusionMatrix(predict(model_nb2, newdata = Test_2), Test_2$icu_yn) 

```
For this model, we find that it has a high sensitivity of 100, which means that every test will be accurate every time it is admissterred. However, this not a good model because it makes the test seem to be 100% accurate, which cannot always happen as there can be cases of a false positive or false negative. 